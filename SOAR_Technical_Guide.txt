Technical Guide to Building the SOAR Mobile App
Introduction
SOAR is a cross-platform mobile app (iOS and Android) focused on emotional wellness and productivity. It
offers  daily  mood  tracking,  personalized  content  recommendations,  AI-driven  voice  podcasts,  a  peer
support  community,  and  app-blocking  for  digital  wellbeing.  The  following  guide  outlines  the  technical
approach for each major feature, covering data sources (APIs), AI/ML integrations, front-end frameworks,
backend architecture, cloud infrastructure, authentication, data security, and essential tools. The goal is to
use  well-supported,  cost-effective,  and  scalable  technologies  to  ensure  a  robust  production-ready
application. 
Tech  Stack  Overview:  We  recommend  using  a  cross-platform  framework  like  Flutter  (Dart)  or  React
Native  to  build  SOAR’s  mobile  app.  These  allow  a  single  codebase  for  iOS  and  Android,  accelerating
development without sacrificing performance. Flutter , for instance, compiles to native code and includes a
rich widget library, enabling high-performance, attractive UIs on both platforms. Native device features
(notifications, sensors, Apple Screen Time API, etc.) can be accessed via plugins or native modules. On the
backend, a  serverless architecture  is advised for efficiency and scalability – using cloud functions and
managed services so we only pay for what we use. This will handle unpredictable workloads gracefully and
minimize ops overhead. Core backend components (databases, authentication, file storage) can be provided
by platforms like Firebase  or AWS Amplify , which offer out-of-the-box scalability and security. We will detail
these choices per feature below.
Daily Mood Check-ins & App Blocking
Feature Description:  Each day, at a user-scheduled time, the app will prompt the user to complete a short
mood survey. Until the check-in is completed, selected distracting apps on the device remain locked  (cannot
be accessed). This “focus lock” integration incentivizes regular mood logging and helps users stay mindful.
After completing the check-in, the blocked apps are unlocked for normal use.
Datasets & APIs:  Mood check-ins are primarily user-generated data (e.g. a rating scale or emoji for mood,
and an optional text journal entry). No external dataset is required for the survey itself. However , to enhance
the experience, the app might incorporate a mood  classification  or sentiment analysis model to interpret
any text the user enters about their feelings. For example, if the user writes a journal entry with their mood,
an on-device ML model (such as a small sentiment analysis model) or a cloud NLP API could classify the
mood (happy, anxious, sad, etc.) to augment the self-reported data. This is optional and would be used only
to provide better personalized feedback. The core functionality (locking apps until survey completion) relies
on device OS capabilities rather than third-party APIs: 
iOS  (Screen  Time  API):  We  leverage  Apple’s  Screen  Time  frameworks  (FamilyControls,
ManagedSettings,  DeviceActivity)  introduced  in  iOS  15.  These  APIs  allow  an  app,  with  user
permission,  to  apply  usage  restrictions  on  other  apps.  Specifically,  the  app  will  use  the• 
1
FamilyControls  framework  to  let  the  user  select  which  apps  to  block  during  focus  time,  and
ManagedSettings  to  “shield”  (block)  those  apps  until  the  check-in  is  done.  The  DeviceActivity
framework  can  schedule  monitoring  (e.g.  each  day  at  the  set  time)  to  enforce  the  lock.  Apple
requires that such usage has a parental control-style permission flow (the user might enable the app
as  a  device  management  or  Screen  Time  provider).  With  correct  entitlements  (FamilyControls
capability and a Device Activity monitor extension), the app can programmatically disable access to
chosen apps. Once the user submits their mood survey, our app will clear the restriction via the API,
thus unlocking the apps. This approach uses Apple’s well-supported mechanism for app blocking,
ensuring we respect user privacy and system rules.
Android:  Android does not offer a first-party “Screen Time API” for third-party apps, but we can
achieve similar results via the Accessibility or Usage Stats APIs. The Usage Stats API  allows the app
to detect the foreground app. Paired with an Accessibility Service , the app can immediately detect
when a blocked app is launched and overlay a fullscreen prompt or redirect the user back to SOAR
until they finish the mood check-in. Apps like AppBlock and Digital Detox employ such techniques to
enforce  focus  mode.  We  will  request  the  android.permission.PACKAGE_USAGE_STATS  and
register an accessibility service; when the user tries to open a blacklisted app, SOAR will bring itself
to the foreground with a reminder . Alternatively, we can integrate with Android’s Digital Wellbeing
features if available (e.g. using Intents to enable system Focus Mode which can temporarily pause
selected apps ). However , using our own service provides more control. Once the daily survey is
completed,  the  app  disables  its  blocking  behavior  for  that  day.  This  method  requires  careful
handling to not overly interfere with the device experience, but it is the most feasible given Android’s
openness. It’s also important to be transparent to the user about needing these special permissions
and only using them for the stated focus-lock purpose.
AI/ML Components:  Mood detection can be enhanced via ML. If the check-in includes a text journal, a
lightweight sentiment analysis model (e.g. a pre-trained mobile-friendly transformer or even a simple word
sentiment lookup) could run on-device to infer emotion. This could validate or supplement the user’s
selected mood (for instance, if the user wrote “I feel overwhelmed and anxious,” the app might tag the
mood as “anxious”). Additionally, over time, an ML model could analyze a user’s mood patterns to predict
trends or detect anomalies (e.g. prolonged low mood) and trigger personalized recommendations or alerts.
These analyses can be done on the backend in a batch process (for pattern recognition) or using built-in
libraries (e.g. Apple’s Natural Language framework for sentiment on iOS). For the focus-lock itself, no ML is
needed – it’s rule-based on completion of survey.
Frontend Implementation:  Using a cross-platform UI toolkit (Flutter or React Native), we will build a Mood
Check-in screen  that appears at the scheduled time (or when the user opens the app). This screen will
present a short survey: for example, a 1-5 mood rating or selection of an emoji, plus an optional text field
for thoughts. The UI will be simple and calming, possibly with a progress bar or motivational text to
encourage honesty. We will use local notification scheduling to remind the user at their chosen time: on iOS,
schedule  a  UNUserNotificationRequest  with  a  daily  trigger;  on  Android,  use  AlarmManager  or
WorkManager to trigger a notification each day. When the notification is tapped, it opens the app to the
survey. Until the user completes it, if they attempt to switch to a blocked app, the frontend (especially on
Android) will immediately redirect them back. On iOS, the blocked apps will show an OS-controlled “Screen
Time” shield by Apple’s API (with a message we can customize, like “Time to check in with SOAR!”). We will
provide a clear flow: after the user submits the survey, show a “Thank you” message and then allow normal
phone usage. The focus lock settings UI  will also be provided in-app, where the user can pick which apps• 
2
to include in the daily block. This can be done via Apple’s FamilyControls UI ( FamilyActivityPicker
presents a list of apps to choose) and via an app list on Android (populated using the PackageManager). We
will integrate the Apple MusicKit authentication here as well if needed (e.g. if the user’s check-in will later
influence Apple Music recs, ensure we have permission beforehand).
Backend & Data:  Each mood check-in entry will be sent to the backend to be stored in a  database . A
NoSQL  cloud  database  (like  Cloud  Firestore  or  Amazon  DynamoDB )  is  a  good  fit  to  store  these
timestamped mood logs for each user . Firestore, for example, is a scalable document DB that syncs well
with mobile apps and supports real-time listeners. We can store each check-in as a document with fields:
userID, date, mood rating, note text, etc. This data is relatively small per entry, but over time provides
valuable history for the user (and for our recommendation engine). The backend can also analyze this data
to detect patterns (e.g. average mood per week). The  serverless  approach means our backend logic for
creating a mood entry can run as a cloud function triggered by a REST API call (for example, an HTTPS Cloud
Function that the app calls when a new check-in is submitted). This function writes to the DB and perhaps
calls  further  logic  (e.g.  update  a  personalized  morning  podcast  content  or  trigger  a  recommendation
refresh based on the new mood). Because the volume of these submissions will scale with user count and
daily usage, using serverless functions ensures automatic scaling – if one morning 10,000 users submit at
once, new function instances spawn to handle the load without performance issues. Data retrieval (for user
viewing their mood history charts, etc.) can either be direct from client (Firestore allows secure direct reads
with rules) or via API calls if using a more controlled access pattern.
Cloud Infrastructure:  We will deploy on a robust cloud platform such as Google Cloud (Firebase)  or AWS .
If using Firebase, the authentication, Firestore DB, cloud functions (via Firebase Cloud Functions), and cloud
messaging (for push notifications) are all tightly integrated. If using AWS, we can set up AWS Cognito for
auth, API Gateway + AWS Lambda for serverless functions, and DynamoDB for data, plus Amazon Pinpoint
or SNS for notifications. In either case, a serverless architecture is cost-effective: we don’t run persistent
servers 24/7, we use on-demand functions and managed databases that scale automatically. For example,
Firebase’s free tier could cover the early stages (some number of writes per day, etc.), and as we grow, we
pay for actual reads/writes and compute time – aligning cost with usage. Hosting  of static content (if any)
can be done via cloud storage (for instance, Firebase Storage for any images the user attaches, though
mood entries likely not images). The cloud also handles the scheduling of notifications: for reliability, we
might use Firebase Cloud Messaging to ensure the daily reminder arrives even if the app isn’t running
(especially on Android where exact alarms might be delayed by Doze mode). The schedule can be stored in-
app for local notification, but a backup push from server at that time can improve reliability. 
User Authentication:  Users will need to create an account to save their mood data securely across devices.
We recommend integrating Firebase Authentication  (or OAuth 2.0  through another provider) for a quick,
secure solution. Firebase Auth provides an easy SDK to support email/password, phone number , and social
logins (Google, Facebook, Apple, etc.). It leverages industry-standard OAuth 2.0 and OIDC under the hood,
meaning we can also allow “Sign in with Apple” on iOS (required by App Store guidelines when other social
logins are present). This ensures a smooth onboarding and the ability for users to sign in on a new device
and  retrieve  their  history.  For  a  more  enterprise  solution,  AWS  Cognito  could  be  used  similarly,  but
Firebase’s developer experience is very streamlined. Either way, authentication tokens will secure all API
calls – e.g. the mood submission function will require a valid user token and will use the user’s ID to store
data under their account.
3
Security & Privacy:  Handling mental health data demands strong privacy protections. All communication
between the app and backend will be over HTTPS (TLS encryption)  so that mood entries or any personal
data cannot be intercepted. Data stored in the cloud database will be encrypted at rest (both Firebase and
AWS encrypt data on their servers by default). We will enforce access controls: each user can only read their
own mood data. In Firebase, this is done with Firestore Security Rules tied to auth; in a custom API, our
endpoints will verify the token and user ID. We will minimize data collection  – only mood and wellness-
related info the user provides – to reduce risk. For example, if the user’s check-in includes an option to share
location (perhaps to correlate mood with location context), we make it opt-in and transparent. The app will
have a clear privacy policy explaining how data is used and protected. On device, any sensitive info (like
cached mood data) will be stored in an encrypted form if possible (both iOS and Android provide secure
storage APIs for small secrets if needed). We will also encourage the user to secure their device (e.g. device
passcode) since personal journal entries might be visible if someone has access to their phone. No mood
data will be shared with third parties  without consent; local analytics will be anonymized if used to
improve features. If SOAR ever connects users with therapists (recommendation feature), that will be done
without exposing the user’s identity – e.g. just showing therapist info for the user to choose to contact. We
will consider compliance with frameworks like HIPAA if we ever integrate with healthcare providers, but as a
consumer wellness app, we still hold ourselves to high standards of encryption and safe storage. Regular
security audits and testing will be done to patch any vulnerabilities.
Additional Tools & Considerations:  For scheduling the daily check-in, we will utilize the device’s scheduling
capabilities (as mentioned, notifications and possibly background jobs). We must handle edge cases: if the
user misses the check-in time or turns off notifications – the app should have a way to trigger the check-in
next  time  they  open  it.  We  should  also  implement  local  reminders  using  calendars  or  widgets  if
appropriate (for user’s benefit). The focus-lock integration needs to be fail-safe: on iOS, if the user revokes
Screen Time permission, our app should detect that and inform the user that app blocking won’t work until
re-enabled. On Android, if the accessibility service is killed by the system, we might prompt the user to
restart it. These scenarios require careful UX so that the core promise (mood check-in before apps) is
reliable. Another tool: analytics can be used (e.g. Firebase Analytics) to track if users are skipping the check-
in or uninstalling – that helps improve the feature. Finally, we’ll implement push notifications  not only for
the daily reminder , but potentially motivational messages – though sparingly, as users might be sensitive.
For development, frameworks like Flutter  have packages such as flutter_local_notifications  for
scheduling notifications and workmanager  for background tasks, and we’d use those. For the Screen Time
API on iOS, we might write a small iOS-specific module or use an existing one if available to interface with
FamilyControls (since that’s a niche use-case, a custom native integration in Swift might be needed). In
summary,  Mood  Check-ins  and  App  Blocking  will  be  implemented  with  a  combination  of  native  OS
capabilities (for enforcement) and cloud services (for data and scheduling), providing a seamless and secure
experience to the user .
Personalized Content Recommendations
Feature Description:  Based on the user’s mood (especially recent check-in), personal profile (age, interests,
mental health history, content preferences), and general usage, SOAR will suggest a curated set of content
across  various  categories.  This  includes:  movies,  books,  YouTube  videos,  local  therapists,  guided
mindfulness  sessions,  journaling  prompts,  relaxing  music  playlists  (Apple  Music),  positive
affirmations,  workouts/yoga  routines,  nutrition  tips  and  nearby  healthy  restaurants,  and  local
events.  The recommendations are meant to improve the user’s mood or maintain positive momentum.
They should feel tailored and relevant – for example, if a user is feeling anxious, the app might recommend
4
a calming breathing exercise, a gentle yoga video, a book about anxiety relief, an upbeat playlist, and
perhaps  highly-rated  therapists  nearby  specializing  in  anxiety.  This  feature  is  content-heavy  and  will
integrate with many third-party APIs and possibly AI models for personalization.
Datasets & Third-Party APIs:  To provide rich and up-to-date content, we will integrate with several well-
supported APIs:
Movies:  We can use The Movie Database (TMDB) API  for movie suggestions. TMDB provides a vast
database of movies with genres, ratings, descriptions, etc., and is free for moderate use with an API
key. It offers structured JSON data for querying movies by mood or keyword. For example, if a user is
sad and needs a lift, the app could query TMDB for feel-good movies or comedies. TMDB is known
for its comprehensive and developer-friendly API, providing details like synopsis, cast, and even
trailers, all in a consistent format. We might maintain a mapping of mood → genre (e.g. “happy”
mood:  show  popular  new  releases  or  inspirational  movies;  “anxious”  mood:  show  calming  or
comedic films). TMDB’s generous rate limits make it feasible to fetch dynamic content for each user ,
but we will cache common requests (like trending movies of the week) on our backend to reduce
calls.
Books:  For book recommendations, the Google Books API  is a good choice. It allows searching for
books by subject, author , etc., and returns details and cover images. Alternatively, we might use the
Goodreads API  (though Goodreads API has some limitations) or an open dataset. Google Books is
free and doesn’t require extensive setup. The app can search for keywords like “mindfulness” or
“inspirational biography” depending on user interest. If the user provided interests (say they like
fiction or self-help), we filter accordingly. We may also maintain a curated list of books for certain
moods (especially if the user has no specific interest info). This list could be stored in our database
and  simply  filtered  by  mood  tag  –  a  simple  solution  if  APIs  are  unreliable.  Initially,  leveraging
Google’s database should cover most queries.
YouTube  Videos:  The  YouTube  Data  API  (v3)  will  enable  us  to  find  relevant  videos  (guided
meditations, motivational talks, relaxing scenery, etc.). The YouTube API allows searching for videos
with keywords and returns data like title, channel, thumbnail, and video ID for embedding. For
example, if mood = “stressed”, we might search YouTube for “10-minute meditation stress relief” and
display a top result. We can also maintain a playlist of vetted videos and just pick from those. But for
dynamic range, using the API is powerful. We will use the YouTube Player IFrame  or a native player
SDK (YouTube provides an Android and iOS player API) to play videos in-app. This requires obtaining
an API key and possibly OAuth if we ever let users connect their YouTube to save favorites (likely not
needed initially). The API usage should be within free quota for standard searches if cached or
limited per user . (We should be mindful that YouTube’s API has quota costs for each request, but
searching a few times per user per day is reasonable.)
Local Therapists:  To recommend nearby therapists, we can use a local business search API such as
Yelp  Fusion  API  or  Google  Places  API .  These  allow  searching  for  businesses  by  category  and
location. Yelp’s API can search for “counseling” or “therapist” in the user’s area (we’ll use the device
location  with  user  permission).  Yelp  provides  business  details  including  name,  address,  phone,
rating , review count, and price level  (which serves as a cost indicator). We can filter or sort by rating
to show highly rated therapists, and possibly by price if the user has a budget concern. The results
can be further filtered by things like “accepts insurance” if Yelp has that info (some do as attributes).• 
• 
• 
• 
5
Each therapist suggestion will display the name, distance, rating (e.g. 4.8 stars), and maybe a “Call”
or “Website” button that opens the Yelp page or dials the number . Privacy note: we are not sending
any user personal data to Yelp aside from location query; and we are not storing which therapist
they tap (unless for analytics). An alternative is Google Places API with a “type=health” or keyword
“therapist”, which could also yield results with ratings and maybe price level. Google’s database
might be broader , but Yelp’s has rich reviews. We could even use both and combine results. However ,
to keep within free usage, using one is sufficient. We will also include an option for the user to
change the location or search radius if needed.
Guided Breathing & Mindfulness:  For guided sessions, we have a few options. We might integrate
with a dedicated meditation API if one exists, but more simply, we can curate a set of high-quality
audio  or  video  sessions.  For  example,  there  are  free  mindfulness  resources  on  YouTube  (as
mentioned) and on platforms like SoundCloud  or specialized services. We might store a library of
MP3 files for various breathing exercises and stream them from cloud storage. Alternatively, services
like  Spotify  or  Apple Podcasts  might have relevant content, but those require the user to have
accounts  or  might  complicate  integration.  A  straightforward  approach:  use  YouTube  for  video
meditations or use content from organizations like UCLA MARC (which offers free guided meditation
audio). We could preload a dozen sessions (short breathing exercises, body scans, etc.) into the app
or cloud, tag them by purpose (sleep, anxiety, focus), and recommend as needed. Over time, if
resources permit, we could generate custom mindfulness sessions via AI or partner with content
creators. In terms of integration, if using YouTube, it falls under the YouTube API approach. If using
preloaded audio, we’d simply stream from our server or bundle a few (though bundling large audio
increases app size, so streaming is preferable). No special third-party SDK needed except an audio
player . 
Journaling Prompts:  Many users benefit from prompt questions to reflect. We can maintain a
repository of journaling prompts (for example, “What is one thing you’re grateful for today?” or
“Write about a time you overcame a fear .”). These can be stored in a static file or database table,
categorized by theme (gratitude, self-esteem, anxiety) or mood. We could also fetch prompts from a
public  API;  indeed,  some  websites  or  public  Google  Sheets  provide  prompt  lists.  However ,
maintaining our own list gives us control. Possibly, we could crowdsource or use an AI model  (like a
GPT-3.5 via API) to generate novel prompts tailored to the user’s situation. But that could be overkill
and costly at scale. To start, a static set of 50-100 quality prompts covering various scenarios, rotated
regularly, would suffice. No user data is sent out here, except if we decide to query an AI externally
(not initially planned). The app will display a prompt of the day (or a few and let the user choose). 
Relaxing Soundscapes / Music Playlists:  For music and sound therapy, integration with  Apple
Music  is highlighted. Apple Music offers an API and the  MusicKit  SDK for both iOS and Android
which allows third-party apps to search the Apple Music catalog and even play full songs (provided
the user has an Apple Music subscription and authorizes access). Our app can ask the user to
connect their Apple Music account. Once connected, we can do things like: search for curated Apple
Music playlists (e.g. “Peaceful Piano” or “Morning Motivation”) and present them. Or we can create
our own playlists via the API. If the user is not an Apple Music subscriber , we might offer 30-second
preview clips (Apple Music API allows that) or fallback to Spotify  if we integrate that (which would be
another OAuth). To keep it simple, we might start with Apple Music on iOS (since many iOS users
have it) and possibly Spotify on Android (since Apple Music on Android exists but is less common).
We will need to register our app with Apple to get a developer token and use MusicKit. MusicKit can• 
• 
• 
6
handle playing the songs within our app, controlling playback, etc. The Apple Music API  can provide
data like playlists, songs, and also personalized recommendations if the user permits (like their “For
You” content) . For our purposes, we might not need user’s listening history, just the ability to
play a known relaxing playlist. We will likely have a few go-to playlists IDs (like some identified calm
playlists). Alternatively, for truly free content, we might use Spotify’s public API  to fetch playlist links
(like there are public “Calm Vibes” playlists). Even if we can’t play directly without Spotify SDK, we
could deep-link the user to Spotify if they prefer . But since the prompt specifically mentions Apple
Music, we’ll focus on that as it’s well-supported on iOS and now has an Android SDK. We will include
a fallback for users who don’t want to connect a music service: perhaps some free ambient sounds
we host (rain, forest sounds) which can be streamed without login. There are free libraries for
ambient sounds or we can license a small number of audio loops.
Positive Affirmations / Quotes:  This content can be provided through an API like  ZenQuotes  or
Affirmations.dev  which return random inspirational quotes. For instance, ZenQuotes.io offers a
simple JSON API that gives a daily quote. We could hit this once a day and display the quote
(“affirmation of the day”). Alternatively, we can store a list of affirmations and cycle through them.
There are many open-source lists of positive affirmations (for different categories like self-worth,
anxiety relief, etc.). Since the cost to store and serve these ourselves is negligible, we might use a
combination: have a local list so we always have something to show offline, but occasionally call an
API for freshness. The advantage of an external API is a larger variety and perhaps famous quotes.
But to ensure no downtime, caching the response is good. For example, call ZenQuotes in the
morning (server side) and broadcast to all users the quote of the day, or call on each client once
(which is also fine, as these APIs are generally lightweight). Many of these APIs are free; if not, we
can find a public domain quote list. (Because showing a nice quote overlay in the app can be
uplifting, we definitely want this feature.)
Personalized Workout or Yoga Plan:  Fitness suggestions should align with the user’s physical
wellness needs. If a user indicates interest in exercise (via initial preferences or maybe connecting to
Apple Health/Google Fit), we can recommend short routines. There are a few approaches:
Use an Exercise Database API  (like API-Ninjas Exercises or the open ExerciseDB) to pull exercises or
routines. For example, API-Ninjas offers an exercise list by muscle group that we could use to build a
small workout. However , a full personalized plan might be complex.
Simpler: have a set of template routines (e.g. a 5-minute morning stretch, a beginner yoga flow, a
cardio routine) possibly in video form (maybe YouTube again has yoga videos we can recommend).
If the user’s mood is low, perhaps a gentle yoga is recommended, whereas if mood is good and
energetic, maybe a more intense workout could be motivating. But caution: for emotional wellness,
we likely focus on gentle exercises that boost endorphins without being too demanding.
We could integrate with Apple HealthKit  or Google Fit  to track if they did the exercise, but that’s
optional. Initially, just providing the suggestion (maybe linking to a YouTube workout video or a
written list of exercises in-app) is enough.
There is also potential to use AI: e.g. feed user preferences into a fitness plan generator model
(some platforms or even GPT-4 could generate a tailored weekly plan). Given the scope, we probably
stick to static content or third-party content.
If we use YouTube for this, we’d search for , say, “10 minute yoga for beginners” or “stretching routine
office break” depending on context.1
• 
• 
• 
• 
• 
• 
• 
• 
7
In summary, minimal API usage here beyond what we have (YouTube) and possibly an exercise info
API if we go that route. 
Nutrition Suggestions & Nearby Restaurants:  For nutrition, we want to give simple diet tips or
food suggestions that can improve mood (e.g. foods rich in omega-3 for brain health, or simply
remind  to  stay  hydrated).  We  likely  will  not  do  calorie  tracking  or  anything  heavy.  A  feasible
approach:
We could use a nutrition API like Edamam  or Spoonacular  which provide recipe ideas and
nutritional info. For example, Spoonacular’s API can recommend recipes (“healthy smoothie recipe”)
and has a food database. However , those often require paid plans after a certain number of calls.
Alternatively, maintain a small set of nutritional tips (like “Have you tried chamomile tea to relax
before bed?” or “Consider a breakfast with protein and fiber to boost energy.”). These can be
triggered contextually (evening vs morning, or based on the mood – e.g. anxious users might get
“avoid excess caffeine” tip).
For nearby restaurants, Yelp API  can be used similarly as for therapists, but with a different query.
We can search for healthy food spots or whatever cuisine the user prefers. Yelp allows filtering by
category, for instance “restaurants, Healthy” or keywords like “vegetarian” or “juice bar”. The
response gives us names, ratings, price, etc.. We’ll show maybe 3 nearby restaurants that align with
healthy eating (within say 5 miles). If location is off or user doesn’t want to share, we might skip this
or allow them to search by city name.
Google Places can also do restaurant search with dietary filters (like using keywords in query). But
since we likely already use Yelp for therapists, we might reuse it for restaurants (just different search
parameters).
We will ensure not to overwhelm the user with too many suggestions; nutrition might be a small
card in the recommendations feed.
Local Events:  To help users engage socially or in wellness activities, recommending events could be
valuable (think local yoga classes, meditation workshops, support group meetings, or even fun social
events if they’re feeling isolated). The challenge is that event data is fragmented:
Eventbrite API  was a common choice, but their public events search API was deprecated. Instead, 
Ticketmaster’s Discovery API  is available and lists a wide range of events (sports, concerts, etc.)
and can filter by location and genre. We could use Ticketmaster’s API to find events in the user’s area
(it has a 5000 calls/day free quota, which is decent). For example, search for events within X miles of
user’s city for the coming week. We might filter by category if we only want certain events
(Ticketmaster has classifications like music, sports, arts, etc.). However , Ticketmaster might not list
smaller community events.
We could also use Yelp Events API  (Yelp has an events endpoint ), though it may not be
comprehensive.
Facebook Events  or Meetup.com  APIs could have relevant data but are harder to work with
(Facebook Graph API requires user login and permissions; Meetup’s API might need an API key).
Perhaps start with Ticketmaster for big events (like “Meditation workshop at local community center”
if listed as an arts event, or “5K charity run” etc.), and supplement with a manually curated feed if we
partner with local wellness centers.• 
• 
• 
• 
• 
• 
• 
• 
• 
• 2
• 
• 
8
Given complexity, we might roll this out later . In the interim, we might have a static list of virtual
events or online webinars for wellness.
But as the prompt includes events, we’ll plan to integrate at least one events source. Ticketmaster
Discovery API can be queried by city name or lat/long, and sorted by date. We’ll present a couple of
upcoming events that might interest the user (maybe even let them choose categories of interest in
their profile to refine this).
In summary, data integration  for recommendations involves many APIs: TMDB for movies, Google Books,
YouTube, Yelp/Google for therapists and restaurants, Apple Music for playlists, quote/affirmation APIs, and
possibly Ticketmaster for events. Each of these requires managing API keys and abiding by terms of use
(we’ll ensure to display any required attributions, like TMDB requires a “Powered by TMDB” credit). We will
implement efficient caching to avoid hitting APIs too frequently – for example, the backend can fetch a list
of top 10 movies in “uplifting” genre once per day and then just pick from that for users rather than calling
TMDB for each user . Similarly, if many users in the same city, we cache local therapist results for that city.
AI/ML  for  Personalization:  Recommending  the  right  content  for  a  specific  user  and  mood  is  an  AI
challenge. Initially, we can use simple  rule-based logic  combined with user preferences. For example, if
user’s mood = sad, we might always include one funny video and one inspirational quote. If their interest =
“movies” we prioritize a movie suggestion; if they hate reading, we might skip book suggestions. These
rules  can  be  handcrafted  and  refined  with  feedback.  However ,  as  data  grows,  we  can  incorporate  a
recommendation system: - A Collaborative Filtering  model could be used if we have many users and we
track which suggestions they engage with. For instance, using Amazon Personalize  or a similar service, we
could train on user-item interactions to predict which content a user may like. Amazon Personalize can train
on implicit data (clicks, watch completions) and content metadata to give real-time recommendations. This
is advanced and might be overkill initially, but it’s scalable. Amazon Personalize is managed, meaning we
feed it data and it hosts an API to get recommendations, with the heavy ML lifted by AWS. - Simpler , we
could  implement  a  content-based  filtering:  create  vectors  for  content  (e.g.  tag  movies  with  “happy”,
“exciting”,  etc.)  and  vectors  for  user  mood  profile,  then  choose  those  with  matching  tags.  Or  use  a
sentiment analysis  on content descriptions to match mood (some projects match movie tone to user
mood via NLP). - Another ML angle is ranking : If we retrieve a bunch of possible items (say 10 movies, 10
videos, etc.), an ML model could rank them in order of predicted benefit to the user . This could be a
regression model or even a small neural network that takes in features (user mood, item genre, user age,
etc.) and outputs a score. Training such a model would require historical data, so likely a later improvement.
- Contextual Bandits  or reinforcement learning could tailor exploration of suggestions over time (to learn
what works for the user). But again, that’s complexity likely beyond MVP.
Given the emphasis on  well-supported tech , we lean on known recommendation engines. For example,
Firebase  has  a  rudimentary  ML  kit  but  not  for  recommendations;  Google’s  Recommendations  AI  is
another managed service like AWS’s. These might be heavy for a small app startup. A pragmatic solution:
start  with  rule-based  and  gradually  integrate  ML.  We  will  instrument  the  app  to  collect  which
recommendations the user likes or uses (e.g. if they click a movie suggestion, we log that). This data
(anonymized, of course) can feed into future model training.
Another ML use here: clustering users  by emotional pattern. If we identify clusters (like a group of users
who often report anxiety in evenings), we can adjust recommendations for them (maybe suggest calming
content in the late afternoon proactively). K-means or hierarchical clustering on mood time-series could be
done offline on our database. This overlaps with the community matching feature (discussed later), but• 
• 
9
from a content perspective, clustering could yield persona types to target with certain content sets. This is
exploratory and not required at launch, but it's a possible enhancement.
Frontend Implementation:  The recommended content will be presented in the app likely as a scrollable
feed  or  dashboard  after  the  user  completes  their  mood  check-in  (or  accessible  anytime  as  a
“Recommendations” tab). A clean UI design might have cards for each category of suggestion. For example:
A movie card  with poster image, title, and maybe “Watch trailer” button (or just a link to details).
A book card  with cover and title/author , maybe “Learn more” linking to Google Books or Amazon.
A video card  with a thumbnail that when tapped opens the YouTube player embedded.
A therapist card  showing name, rating, distance, “Call” or “View on Map”.
A mindfulness session card  (“5-minute Breathing Exercise – Tap to Begin”) that when tapped plays
audio or navigates to an exercise screen.
A journal prompt card  with the prompt text and a “Write now” button that opens a journal input
screen.
A music playlist card  with album art or playlist cover , and a play button using Apple Music SDK if
authorized.
A affirmation card  maybe just showing the quote on a nice background.
A workout card  with a title (e.g. “Morning Stretch Routine”) and a play icon to start video or list of
steps.
A nutrition tip card  with a short tip or recipe suggestion.
An events card  listing one or two upcoming events (with date/time and title, tapping could open a
web link or details page).
These cards can be organized by relevance. Possibly highlight a couple that are most recommended and
show others smaller . We will likely allow the user to customize what types of recommendations they want to
see (some users may not want workout suggestions, or already have a therapist, etc.). A settings page can
let them toggle categories.
From a development perspective, frameworks like Flutter make it easy to create these rich UI components.
We might use a ListView  of differently styled cards. Some will require dynamic loading – for instance, the
YouTube thumbnail URL, or images from TMDB (TMDB gives image paths which we form into URLs to their
image  server).  We’ll  utilize  caching  for  images  (Flutter’s  cached_network_image  or  similar  to  avoid  re-
downloading posters every time). For Apple Music, MusicKit can fetch artwork for playlists. We should
adhere to design guidelines of these services (Apple has rules for using their content – e.g. displaying an
Apple Music badge if we let them sign up through our app). 
Interactivity: if the user taps a movie, we might show more details and a link to watch (could deep-link to a
streaming service if known – TMDB can give you if it’s on Netflix, etc., but that’s more integration. Perhaps
we just link to TMDB’s page or Google search for the movie). A therapist card tap could open Google Maps
at that location or dial their phone via intent. We want to minimize jumping out of our app too much, but
for booking a therapist or visiting a restaurant, eventually they leave the app – that’s fine.
Backend  Architecture:  The  backend  will  play  a  crucial  role  in  aggregating  and  filtering  these
recommendations. Each of the external APIs require keys that we don’t want to expose in the client app. So
rather than calling them directly from the mobile app (except maybe YouTube or Apple Music which might
have client-side SDK flows), we will have our backend act as a proxy . For example, our serverless function• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
• 
10
could have an endpoint /getRecommendations  which, given the user’s mood and profile, will internally
call the necessary APIs and build a response JSON containing all the suggestions. The mobile app then
receives this JSON and displays the content. This way, our keys for TMDB, Yelp, etc., remain secret on the
server side. It also reduces the number of network calls the client has to make (one call to our server vs.
hitting 5 different APIs from the phone). The downside is our server function has to do multiple calls which
might be slightly slow; we can mitigate by doing some calls in parallel and caching results when possible.
Using a microservices  approach, we might even split this: e.g. a “Recommendations Service” that handles
these content fetches, separate from the core app logic. But at early stage, a single cloud function can
orchestrate it. If it becomes slow, we could have dedicated functions for categories (one for movies, one for
therapists, etc.) and trigger them concurrently (though combining results then is tricky unless the client
does it).
We might also pre-compute some recommendations. For example, every morning, a scheduled job could
generate the day’s affirmation, quote, maybe pick a featured video, etc., and store them. Then the user’s
request just pulls those plus any user-specific ones. This hybrid approach can improve performance.
The  database  will  store  user  preferences  (interests,  disliked  categories,  etc.),  and  possibly  previous
interactions (to avoid repeating content too often – e.g. don’t suggest the same movie trailer they watched
yesterday). We may also store a catalog of content: for instance, a collection of mindfulness sessions our
team has curated (with IDs or file links). Having our own content repository in the DB allows mixing it with
live API results.
Cloud Infrastructure & Scaling:  Because recommendations involve potentially heavy API usage, we have
to design for scaling and cost. Using serverless means if 100 users request recommendations at once, and
each triggers calls to external services, our function concurrency will spike. We ensure our cloud account
has sufficient quota (Firebase functions auto-scale up to a limit, AWS Lambdas similarly). We may need to
use caching layers  – possibly an in-memory cache like Redis (managed, e.g. AWS ElastiCache) if some calls
are very frequent. However , given personalization, caching is mostly useful for common data (like top 10
movies globally doesn’t change hourly). We should also consider  rate limits  of APIs: e.g. Yelp’s free tier
might allow a certain number of calls per day. We may aggregate requests or fall back gracefully if limits hit
(like if Yelp fails, maybe just don’t show therapist suggestions rather than error).
Storing content: if we host any media (like audio files for meditation), we’ll use cloud storage (Firebase
Storage or S3). These integrate easily with our frameworks and can serve content via CDN. For images from
third parties, we typically just load from their URLs (TMDB has a CDN for images, Apple Music provides
URLs, etc.), which is fine.
Authentication & Personal Data Handling:  The rec engine uses personal data like age or mental health
history.  This  data  will  be  part  of  the  user’s  profile  stored  securely  in  our  database  (likely  in  an
authenticated Firestore document  or Cognito profile). Access to it is only on a need-to-know basis (e.g.
the recommendation function reads the age to decide content rating for movies – if under 18, filter out R-
rated movies, for instance). Mental health history (like if user indicates they have depression or anxiety
issues) is highly sensitive. We will use it only to tailor recommendations (for example, if they have severe
anxiety,  we  may  prioritize  therapy  or  anxiety-specific  content).  This  processing  can  be  entirely  on  the
backend without sharing that flag to any third party. We must ensure any such data is not leaked in logs or
analytics. Possibly encrypt those fields in the database for extra security (on Firebase, data is encrypted at
rest by default, but we could double encrypt certain fields client-side if extremely sensitive).
11
When calling external APIs, we will not send any personal identifiers . For example, when searching Yelp
for  therapists,  we  use  location  but  not  the  user’s  name  or  anything.  When  using  Apple  Music
recommendations, Apple’s API might implicitly use user’s subscription context to give recs, but that’s under
Apple’s privacy domain (and user consents by connecting). Our usage should comply with each API’s policies
(e.g. not storing Apple Music content data permanently, etc.). 
Additional Tools:  To manage the multiple integrations, using a API orchestration library  or cloud function
templates helps. For instance, Node.js with axios to call APIs concurrently, or Python with  aiohttp  for
async calls. We’ll also employ monitoring  – e.g. track latency of the recommendation function, log if any API
fails or responds slowly. This helps maintain a good UX by possibly skipping or defaulting content if a
provider is down. 
Push  notifications  might  also  be  used  here:  perhaps  the  app  can  send  a  “Here’s  your  daily  dose  of
positivity!” notification linking to the recommendations screen, especially if the user didn’t open after a bad
mood entry (just an idea – but careful not to annoy).
Finally, ensuring  content safety : we should vet that the suggestions we provide are safe for a mental
health context. For example, if using an automated approach, an inappropriate YouTube video shouldn’t slip
through  (we  can  use  YouTube  API’s  safeSearch  parameter  to  avoid  explicit  content).  Also,  therapist
recommendations – we should clarify we are not endorsing them, just showing information (maybe link to
their reviews). It might be wise to add a disclaimer in-app for that. For quotes, ensure they are actually
positive (some quote APIs might include sad quotes – we may filter manually or choose sources focused on
positive affirmations). Using ML moderation  APIs (like Google Perspective for toxicity) on content might be
overkill, but the team can manually curate sources to be sure.
In summary, Personalized Recommendations will be implemented via a combination of  rich third-party
content sources  and smart personalization logic . The architecture centers on a serverless backend that
fetches and fuses data, while the frontend presents it intuitively. This modular approach (each content type
via its API) means we can add/remove categories easily and scale the feature horizontally (if one API
becomes problematic, it won’t break the others). By prioritizing popular , well-documented APIs like TMDB
and  leveraging  managed  services  for  heavy  lifting  (like  potential  use  of  Personalize  for  scaling
recommendations), we ensure the solution is robust and maintainable.
AI-Generated Voice Podcasts
Feature Description:  SOAR will provide a daily podcast  – a short audio narrative (perhaps 3-5 minutes) –
generated  by  AI  and  tailored  to  the  user’s  current  mood  and  needs.  The  idea  is  to  give  the  user  a
comforting, human-like voice experience each day, without the cost of producing actual podcast recordings.
The content of this mini-podcast could include: a recap of the user’s mood trend (“It seems the past few
days have been a bit stressful – remember it’s okay to slow down…”), positive affirmations or quotes,
personalized  tips  (like  “today,  consider  taking  a  short  walk  outside”),  and  highlights  of  recommended
content (“we found a calming piano playlist you might enjoy”). The narration will be in a  soothing AI-
generated voice , designed to be pleasant and empathetic.
AI/ML  Components:  This  feature  heavily  involves  Natural  Language  Generation  (NLG)  and  Text-to-
Speech (TTS) : - We need to create the script (the textual content) of the podcast for each user each day. We
12
can achieve this either by templating or using a generative AI model. A templating approach might have
pre-written segments that we fill in based on user data. For example: “Good morning [Name]! It’s [Day of
week].  [If  mood  yesterday  was  low:  I  noticed  you  felt  a  bit  down  yesterday.  Today’s  a  new  day…
encouragement]. Here’s an affirmation for you: [insert quote]. And remember , [insert general wellness tip].
Have a great day!” This can be done with simple logic and a repository of lines to choose from. For more
advanced personalization, we might integrate an NLP model like GPT-3.5 via OpenAI’s API to generate a
unique message given a prompt of the user’s context. For instance, we could prompt: “User’s mood: anxious.
Generate a 2-minute encouraging script with a motivational tone, including a breathing exercise reminder.”  The
model could produce something nicely phrased. However , relying on an external API like OpenAI for every
user daily might be costly and raises privacy questions (we’d be sending some user mood info to the AI).
Also, consistency in tone is important – we want the podcast to have a familiar style. So a balanced
approach: possibly use templated base and allow an AI to fill in one creative line (like a unique motivational
sentence). - The voice generation is the main ML component. We have a few strong options: - Cloud TTS
Services:  Leading cloud providers have high-quality neural TTS. For instance, Google Cloud Text-to-Speech
offers 380+ voices with humanlike intonation (WaveNet and newer models). We can choose a particularly
soothing voice (some are specifically labeled as calm or narratively friendly) and synthesize audio from our
script. Google’s voices are very natural, as are Amazon Polly’s Neural voices , which likewise provide lifelike
speech  in  many  languages.  Both  services  are  pay-per-character  (with  free  tiers  for  small  usage).  For
example, Polly charges about $4 per million characters for neural voices after free tier . A 5-minute script
might be ~750 words (~3750 characters). For 1000 users, that’s 3.75 million chars – about $15, which is
reasonable. As we scale to say 100k users, cost increases, but hopefully we have revenue by then to cover or
we optimize (maybe not every single day per user if cost is high). - Edge / On-Device TTS:  We could also use
the device’s built-in TTS (both iOS and Android have system TTS engines) to generate the voice on the
phone. This avoids cloud costs and privacy issues. However , default voices are not as “soothing” or flexible
as neural cloud ones. There are some on-device neural TTS options (Android’s newer Speech Services by
Google, Apple’s Siri voices). Apple, for instance, has an API for custom VoiceProvider but not sure if it allows
custom voice fonts. Probably using cloud for quality is better . - Custom Voice Model:  If we wanted a unique
brand voice, one could train a model or fine-tune a voice clone (e.g. ElevenLabs, Azure Custom Neural
Voice). That might give a distinct voice but requires a lot of effort (recording sample data, etc.) and might be
unnecessary given the many voices available. Amazon Polly offers many voices and even a neural voice that
we can brand to an extent, and does not retain content after conversion for privacy . - We will likely pick
a female or male voice that tests show users find soothing. We can even let the user choose between a
couple of voice options (some might prefer a male voice, etc.). - The pipeline: The backend will take the
prepared text, send it to the TTS API (e.g. Google or Amazon) via SDK or REST, and get back an audio file
(MP3). We will then deliver that file to the app. We could do this on-demand (when the user hits “Play daily
podcast”) or pre-generate it early in the morning and notify the user it’s ready. Pre-generation is user-
friendly (no wait time) but means generating for everyone regardless of whether they listen, potentially
wasted compute. On-demand means a 1-5 second wait for TTS to respond, which might be okay with a
loading spinner . A compromise: generate for active users (those who have listened frequently) in advance,
others on-demand.
Datasets & Content Sources:  Aside from the user’s own data (their mood logs, preferences) which drive
the content, we’ll use some of the same content datasets from recommendations: - The daily affirmation/
quote included in the podcast can be pulled from our quote source (as discussed). - We might include a
“word of the day” or “fact of the day” if we think it adds value – those could come from public APIs too, but
not required. Perhaps keep it focused on emotional wellness facts if anything. - If the user has a streak of
journaling or mood check-ins, we can mention that as positive reinforcement (“You’ve checked in 5 days in a
13
row, great job keeping track of your feelings!”). - The length of the content is short enough that we don’t
need a large dataset; it’s more about assembling relevant pieces.
Frontend Implementation:  The app will have an audio player interface  for the daily podcast. This could
be on the home screen as “Today’s Podcast” with a play/pause button and a waveform or progress bar . It
should show maybe the date and a title (“Your Daily Lift”) and possibly a subtitle like “AI-guided reflection”.
The user can play, pause, seek. We can allow them to share it or save it if they found it really nice (though
since  it’s  personal,  sharing  might  not  be  applicable,  but  maybe  they  want  to  save  a  particular  quote
snippet). 
We’ll implement the player using native capabilities: on iOS, AVAudioPlayer or AVPlayer can play the MP3
stream; on Android, MediaPlayer or ExoPlayer can be used. In Flutter , we can use the just_audio  plugin
which wraps these nicely. The audio can be streamed from a URL (if we stored the MP3 in cloud storage) or
downloaded once then played. Streaming is fine if the file is not too large (a few MB), and we can also allow
offline by caching the last few podcasts on device. 
When the user triggers play, if we haven’t preloaded the file, the app might call the backend, which triggers
generation and returns the file or URL. We’ll need to handle this possibly asynchronous generation. A user
expectation might be to hit play and it starts within a second or two. If our backend pre-generated at, say, 5
AM and stored the file, then at 7 AM when user wakes up, it’s instant to fetch (just a cloud storage URL). If
on-demand, a slight delay is okay but we should show “Preparing your podcast…” to set expectations.
We  should  also  integrate  background  play/pause  (so  they  can  listen  while  screen  off)  –  that  means
integrating with the OS background audio controls, notification player on Android, Control Center on iOS.
That’s doable with existing audio packages. 
Backend & Cloud Workflow:  For daily automation, we can use a  Cloud Scheduler (cron)  to trigger a
function every day at a certain time (or even per user timezone if we store that, but that’s complex; more
likely we generate when needed). If doing on-demand: a request to /dailyPodcast?userId=XYZ  will: 1.
Retrieve the user’s latest mood and any relevant data (from the database). 2. Compose the text script.
Possibly this is done via a template engine or by calling an AI service (with proper prompt). If calling an AI
like GPT, we would include no personally identifying info, just mood summary. But likely we’ll avoid external
NLG at first and use our internal content. 3. Call the TTS API with the text. For example, using Google TTS:
send an API request with the text and voice parameters (language, gender , etc.). Get back an audio content
(the API can return the binary or a link). 4. Store this audio file. We can either send it directly to the app in
the response (as a binary stream) or upload it to a cloud storage bucket and return a URL. Storing might be
useful if the user wants to re-listen later without regen. But storing every user’s daily file is also storage
heavy (1000 users * 30 days = 30k files; though they’re small mp3s so maybe fine). We could cleanup old
files periodically or generate file on the fly each time (they might not notice small differences). 5. Return the
URL or audio to the app.
We must secure that endpoint so one user can’t request another’s podcast (use auth token and check
userId). Also, because voice generation uses potentially sensitive mood info, ensure the text or audio isn’t
logged by our servers. Cloud providers state they don’t retain the text or audio content for user requests
beyond processing, especially if we use standard voices, so that’s good.
14
If pre-generating: we’d need to schedule at user’s usual wake time or so. We could simplify by generating
for everyone at like 4 AM UTC daily, but that would ignore timezones. Alternatively, generate when they
complete the mood check-in: as soon as they submit their mood in the morning (if that’s their routine), kick
off generation so it’s ready by the time they might want to listen (maybe immediately). That might align
nicely: after mood check-in, “listen to today’s brief podcast” as an option.
Cloud Infrastructure Considerations:  TTS API calls are heavy on CPU (for the provider) but trivial for us
(just an API call). They are synchronous but can be a few seconds long for multi-second audio. We should
allow our function a slightly higher timeout (maybe up to 10 seconds) to be safe. We might also parallelize if
adding background music or mixing audio (not likely; probably keep just voice for now, or voice with slight
background music – if we wanted that, we’d have to mix an audio track with the TTS output using an audio
processing library on server or client, which is extra complexity). We will likely stick to plain voice or let the
voice output include some “humming” if needed (some TTS can include background but not usually).
Storing audio on cloud: We can put them in an Amazon S3 bucket  (if AWS) or Firebase Storage  bucket.
These can serve files via CDN URLs. We’d secure it such that only the user (with a token or difficult-to-guess
URL) can access their file if privacy is a concern. However , since the content is not extremely sensitive (it’s
somewhat generic encouragement, albeit personal mood references), and it’s short-lived, we might not
worry if someone somehow got the URL (which is unlikely). If using Firebase, we can require Firebase Auth
to download the file (security rules by file path including userId).
User Authentication & Privacy:  The user doesn’t explicitly log into this feature separately; it’s part of their
account. The content is personal because it may reflect their mood (“you’ve been feeling anxious”). We treat
the generated audio as personal data. So all privacy measures stand: encrypted in storage, not shared. If
using external AI for text generation, we’d mention in privacy policy that we send mood context to an AI
service. But if we avoid that, all the better .
Additional Tools:  - We will incorporate  analytics  to see if users play the podcast fully, as that indicates
value. We can track events like “PodcastPlayed” and “PodcastCompleted” to gauge engagement. - If users
like these, maybe allow feedback or rating (“Was this helpful?”) via simple thumbs up/down to improve
content selection. - Possibly give user control: maybe they can set when they want to receive it or if they
want certain content in it. But initially, it can be automatic daily or triggered by check-in.
Notification:  We could send a push notification when the podcast is ready: “🎧 Your personal daily
podcast is ready – tap to listen!” This will increase awareness of the feature. Using FCM or APNs to
send that each morning to users who enabled it (opt-in likely).
Voice customization:  If feasible, we might incorporate minor voice emotion controls. For instance,
some TTS allow adjusting speaking rate or tone. If user is anxious, maybe a slightly slower , calmer
speech. If they are sad, a gentle empathetic tone. We can pick voice variants accordingly (some APIs
have “narration” vs “cheerful” styles). We’ll experiment and choose what seems universally calming.
Overall, the AI Voice Podcast feature uses state-of-the-art AI voice tech to deliver a unique, scalable form of
daily user engagement. By using managed services like Google TTS, we avoid maintaining complex ML
models  ourselves,  while  still  delivering  near-human  quality  speech.  This  should  delight  users  and
differentiate the app, at a reasonable cost and effort.• 
• 
15
Community Feature (Peer Support Group Chats)
Feature Description:  The community feature allows users to opt in to connect with others who have similar
emotional patterns or history. The goal is to facilitate peer support: users can join group chats to share
experiences, encourage each other , or just talk, knowing others “get it”. We will likely form groups based on
certain criteria (e.g. one group for people dealing with anxiety, another for people with ADHD, or perhaps
group by age range or mood trends). It’s important this is opt-in  and moderated for safety. Users who join
can text chat in the app with the group and possibly use nicknames to maintain some anonymity if desired.
Datasets  &  Matching  Algorithm:  We  don’t  use  a  third-party  dataset  here,  but  we  do  use  the  user-
generated data (mood history, profile)  to match users. How to match: - Simplest: let the user choose
communities of interest. For example, during onboarding or in settings, they could tick “I’m interested in
anxiety  support,  depression  support,  productivity  tips,  LGBTQ+  support,  etc.”  We  then  place  them  in
corresponding chat groups. This method relies on self-selection rather than algorithmic matching, which is
straightforward and ensures common interest. - Additionally, or alternatively, we can algorithmically match
by  emotional patterns . For instance, we analyze mood logs and find users who frequently have similar
mood trajectories. A statistical way could be computing correlation between users’ mood time series or
clustering as mentioned earlier . If two users both tend to feel down on Sunday nights and anxious Monday
mornings, that’s a pattern. But matching on such specifics might be overkill. Instead, we might categorize
users broadly: “mostly happy”, “cyclothymic (ups and downs)”, “even-keeled”, “often anxious”, etc. Then
group those who often report anxiety, for example. This aligns with grouping by mental health history if we
have that info (like if they indicated they have social anxiety, group them with similar). - Because we want
group  chats  (not  just  one-on-one  matching),  we  don’t  need  perfect  pairings;  we  need  a  reasonable
grouping. Likely group by topic is best (like a support forum style). - The group sizes should be moderated,
maybe 20-50 people per group, to feel active but not too crowded. We can scale by creating multiple groups
for popular topics as needed (like Anxiety Group 1, 2, etc.). - Initially, we might start with a few broad
groups: e.g. “General Positivity”, “Anxiety Support”, “Productivity Buddies”. This invites users to join based
on interest rather than any hidden algorithm. The “similar emotional patterns” from the prompt can be as
simple as those who have indicated similar struggles. - We do not have an external API for matching; it’s
internal logic. If we wanted, we could use an ML clustering on user’s mood vectors to find clusters (like
principal component analysis then k-means to identify say 5 clusters) and label those clusters in some
human-friendly way to form groups. This is a neat data science project but might not drastically improve
over self-selection.
Communication Platform:  The crux is enabling real-time group chat in the app. We have options: - Build
our own  via a backend with WebSockets or real-time database. For instance, using  Firebase Realtime
Database or Firestore  in real-time mode is a quick way. We can create a “rooms” collection and “messages”
subcollection. The app listens for new messages via real-time listeners, and we use Firebase’s sync to get
near-instant updates across users. Firestore can handle quite a number of concurrent users, but cost can
rise  with  heavy  messaging  (each  message  is  a  write/read).  For  a  moderate  community,  it’s  fine.  -
Alternatively, use a  managed chat service  or library like  Sendbird, Stream, or Twilio Conversations .
These are purpose-built for in-app chat and often provide SDKs, moderation tools, etc. They have free tiers
and paid as you grow. For example, Stream Chat SDK can give us a pre-built solution for group chat (with
message threads, reactions, etc.). The trade-off is cost and reliance on a third party. But given our time,
using Firebase might be simpler since we already incorporate it. - If using AWS, one might use  AppSync
(GraphQL)  with subscriptions for realtime, or even AWS IoT Core for pub/sub chat – but those are more
complex. AWS also has an Amazon IVS Chat or Chime SDK for messaging in some contexts, but that’s not
16
widely used for generic chat. - For an MVP, Firebase  is likely the fastest path: we can store messages with
fields: senderId, text, timestamp. The client listens and displays in a chat UI. We can use an open-source UI
component (there are libraries for chat UIs in Flutter or RN so we don’t have to design bubbles from
scratch). - We have to implement typing indicators, online status  maybe, which are nice-to-have. Those
can be done with presence detection (Firebase has a presence system where you set a user’s status on
disconnect). - Notifications for new messages:  We want users to be notified if someone mentions them or
in general if there’s activity in their group (with option to mute). We can use Firebase Cloud Messaging to
send push notifications on new messages. Possibly the backend could trigger Cloud Functions on new
message writes to send notifications to group members. Or we rely on the client to do periodic checks –
push is more real-time. - Moderation & Safety:  This is crucial in any mental health related community. We
should implement basic  community guidelines  and have a way for users to report messages or users.
Initially, this might be manual (reports go to our team). We can later employ AI moderation: e.g. run
messages  through  Perspective  API  to  flag  toxicity  or  self-harm  indications.  But  real-time  automated
moderation is complex to perfect. At least filtering profanity might be done by a simple bad-word list on
client side (or let them through but with asterisks). - We should also ensure users are aware this is not
professional counseling, just peer support. Perhaps disclaim that and encourage seeking professional help
if they express crisis situations. - If a user seems to indicate self-harm or crisis, ideally the community or a
bot should respond with resources (like “If you are feeling suicidal, consider reaching out to ...”). This could
be triggered via a simple keyword scan or an ML sentiment analysis on messages in the backend. This
might not be in initial version but is a consideration.
User Authentication & Privacy in Community:  Users might not want to post under their real name. We
can allow them to choose a display name  or alias for the community. This alias can be stored in their profile
(maybe default to something like “User1234” if they don’t set it). That way, in chat we show alias and maybe
an avatar (they can set an avatar image if desired). We will protect their actual identity (email, full name)
from other users. - Only people in the same group can see their messages. We enforce security rules (for
instance, if using Firebase, each message is under a group node and we ensure only members of that group
(those userIds listed) can read). - Joining a group: We will have an interface for opting in. Possibly a list of
groups they can join. On join, backend adds their userId to that group’s member list and from then on their
app gets messages from that group. - We should decide if one user can join multiple groups – likely yes, if
they have multiple interests (maybe they want both Productivity and Anxiety groups). The app should allow
toggling between group chats. - There’s also a possibility of direct one-on-one chat (like find a “buddy”).
That’s not explicitly asked, but maybe down the line. For now, focus on group.
Backend Architecture:  - If using Firebase: minimal backend needed for the messaging itself, since clients
can write/read directly. But we might use Cloud Functions for things like: - On new message, check for
disallowed content, maybe auto-moderate or send notifications. - On join group, maybe send a welcome
message via a function or update some counts. - On user leaving group or deleting account, cleanup their
messages or mark them anonymous. - If using a third-party chat service, their servers handle the backend,
we just configure through their API keys.
We might need a User Directory  for community to see who is in their group (maybe a list of members with
alias, maybe mood icons if they choose to share current mood). We could show basic profile (alias, maybe
an emoji representing their general mood or one thing they love e.g. “Dog lover”). These are nice touches if
time permits.
17
Cloud Infrastructure:  For scaling, real-time chat can produce many small operations. Firestore scales to
millions of messages, but cost should be considered (every message is a write and a read for each user
online). We can mitigate cost by using Firestore’s new Stream feature  or aggregating messages (though
not likely needed unless extremely active). If usage grows massively, moving to a dedicated chat service or a
socket server might be cheaper at some point, but initially, using existing infra saves dev time (which itself
is cost).
Security:  We touched on it – ensure group membership is required to read messages. Also, while chat
messages might not be highly sensitive in general, health contexts can be (people may share personal
struggles). We will treat chat data as sensitive: store it securely (encryption at rest by default on Firebase/
AWS). Consider enabling message deletion/expiration : maybe messages older than X days are wiped to
protect privacy and reduce data load. That can be done with Firestore TTL or a cron job.
Additional  Tools:  -  Notification  scheduling  for  group  events  (discussed).  -  Possibly  integration  with
phone’s  contacts  or  social sharing  if user wants to invite a friend to the app or to their group (should
ensure that’s user-driven). - We might integrate  mention  functionality (like @username in chat) which
triggers a targeted notification. - Use rich text  or emojis in chat – likely yes to emojis. Possibly allow sharing
images/GIFs for emotional expression. That requires more storage (upload images via Firebase Storage,
etc.). We might postpone image messages for simplicity, focusing on text initially.
Analytics : track engagement in community, and whether it correlates with mood improvements.
In summary, the Community feature will be built on a reliable messaging backbone (likely Firebase for rapid
development). We will prioritize user safety and privacy by controlling access and providing moderation.
The technology choices here (cloud real-time DB vs dedicated chat service) weigh development speed vs
scalability, but for a production-ready solution that can scale, Firebase (backed by Google Cloud) should
handle a considerable user base while being cost-effective (pay-as-you-go, with free tier to start). The
serverless  nature of using Firebase means we don’t manage a persistent chat server and can focus on the
app experience. 
By matching users in supportive groups, we hope to foster a sense of community. We’ll monitor these
spaces closely in early stages to ensure they remain positive and adjust our matching algorithms (or group
definitions) if needed. This human-centered approach, supported by robust cloud tech, will let users help
each other , which can be incredibly empowering.
App Blocking (Focus Lock Integration)
(This feature was discussed in tandem with Daily Mood Check-ins, but here’s a focused summary.)
Feature Description:  SOAR will lock access to selected distracting apps on the user’s phone each day until
the user completes their mood check-in. This creates a “focus lock” that encourages users to take a mindful
moment before diving into potentially stressful social media or games. The user can configure which apps
to lock and the time of day the lock is enforced (e.g. after waking up until check-in done, or perhaps in
future, during work hours for periodic check-ins). 
Technical Implementation:  On iOS, we utilize Apple’s Screen Time APIs (specifically the FamilyControls and
ManagedSettings  frameworks)  which  enable  third-party  apps  (with  appropriate  entitlements  and  user• 
18
consent)  to  impose  usage  restrictions.  Our  app  will  present  the  iOS  system  dialog  for  Screen  Time
permission (this requires the device to be in a Family setup or the user to authorize our app as a device
management for themselves – Apple has a flow for “self” parental control in iOS 15+). Once granted, we can
get  opaque  tokens  for  the  apps  the  user  selects  to  block.  Using
ManagedSettingsStore.shield.applications  we apply a “shield” (an overlay that Apple’s system
shows) on those apps. We also schedule a DeviceActivity event for the daily time to automatically re-lock if
needed. When the user finishes the check-in, we call store.clearAllSettings()  to remove the shield,
thereby unlocking the apps. We also handle temporary unlocks if needed and relock after a period – Apple’s
API allows scheduling (e.g. if user tries to cheat, we could re-lock quickly, but since it’s tied to check-in
completion, it’s straightforward). The integration must be carefully tested on device because the Screen
Time entitlement requires a special provisioning profile and cannot be tested on Simulator . But it’s a well-
documented approach and Medium guides have demonstrated building such app blockers.
On  Android , lacking an official API, we implement a workaround: an  Accessibility Service  that monitors
app launches. In our service’s onAccessibilityEvent , we check if the foreground window’s package is
one of the blocked apps (we maintain that list in SharedPreferences or fetched from our server config). If
yes, we immediately launch an overlay activity that covers the screen, effectively preventing usage of the
blocked app. This overlay will display a friendly reminder like “Hold on! Complete your SOAR check-in to
unlock this app.” The overlay activity could be a transparent activity with a simple dialog that cannot be
dismissed easily (perhaps the back button goes home). This is similar to how certain parental control apps
function. We also use the Usage Stats API  to get daily usage or app open events; usage access permission
helps to identify the app in foreground in case accessibility is not 100% reliable. Another technique is to use
Device Admin (DevicePolicyManager)  with a work profile or something to restrict apps, but that’s typically
for enterprise and might require the app to be a device owner , which isn’t feasible for a normal user device.
So accessibility is the user-friendly way. We must instruct the user to grant the Accessibility permission
(which shows a warning about observing actions – we explain why and ensure we only use it for focus lock).
We also handle the scenario that the user might disable the service – maybe send a notification reminding
them focus lock is off.
To schedule the lock, we can simply enforce it at the notification time of the check-in. Before check-in is
done, the service is actively checking. After check-in, we set a flag so the service knows not to block. If the
user tries to use a blocked app during the lock period, our overlay triggers. If they never tried, nothing
happens aside from maybe a persistent notification saying “Focus Lock active”. We can also integrate with
Digital Wellbeing  via Intent (Android has a hidden API to launch focus mode settings; we might not rely on
that). Our approach should work universally. It does require the app to be running (the accessibility service
runs in background once enabled). This can slightly affect battery, but we will optimize it by mostly sleeping
until an app launch event occurs.
Necessary APIs and Tools:  No third-party APIs here, but platform-specific frameworks: - iOS FamilyControls
and related Screen Time APIs (we will follow Apple’s guidelines and test thoroughly; also handle if user
revokes permission mid-use). - Android AccessibilityService (with proper declarations in manifest and an
accessible user flow to enable it). - Possibly Android System Alert Window permission if we use a system
overlay window. But using an Activity with a special launch flag (TYPE_APPLICATION_OVERLAY) may suffice.
We will try to avoid the deprecated draw-over-other-apps approach if possible, in favor of a full-screen
activity on top. - Scheduling: use Android’s AlarmManager or WorkManager to start the check-in notification
and enable blocking at the given time daily. iOS scheduling is handled by DeviceActivity schedule or simply
by notification + logic when app opens.
19
Frontend:  The user experience of focus lock is mainly the interruption when they try to open a blocked app.
On iOS, the system overlay automatically shows a default message and an option “Ignore Limit” if we allow
it (we might not allow ignore without a passcode or something to ensure they really do check-in). On
Android, our custom overlay will be part of our app’s UI – we should make it clear and not too harsh.
Possibly show their lock screen (like “Before using XYZ, take a deep breath and check in with yourself via
SOAR.” with a button “Open SOAR to Check-in”). Pressing that will bring them to the mood survey screen.
We  need  to  ensure  they  cannot  circumvent  easily  (savvy  users  could  force-stop  our  app  or  disable
accessibility – we can’t fully prevent that, but our target users want this feature for their own good, so it’s
okay).
In the settings, the Focus Lock configuration UI  allows selecting apps (in iOS we present Apple’s picker UI
which lists apps categories etc.; in Android we’ll list installed apps with checkboxes, possibly filtering out
system apps). Also allow scheduling the daily lock time and which days (maybe weekends off if they want).
This schedule data we’ll use for notifications and enabling the lock logic. We will also provide an override: if
user really needs to bypass, they could return to SOAR app and complete check-in or we may allow a bypass
code if absolutely needed (on iOS Screen Time, they could just remove our profile, but on Android maybe a
secret way to disable for emergency).
Backend:  There’s not much backend involvement here since it’s device-centric. Perhaps we might log events
(like how often they bypass or how often they complete on time, for analytics). But all enforcement is local.
We could store their chosen list of blocked apps on our server if we want them backed up, but it might not
be necessary or even possible to identify apps across devices (different OS have different app identifiers).
Security & Privacy:  Focus lock deals with controlling other apps. On iOS, Apple’s API is designed with
privacy – we don’t actually get data about what the user does in those apps, just tokens to block them. On
Android, the accessibility permission technically gives us a lot of power (it can read screen content, perform
gestures, etc.), but we will explicitly not collect any of that data. We’ll limit the service to just monitoring the
app name. We’ll include that in our privacy policy, assuring we do not store or transmit any usage data from
accessibility. This is crucial because accessibility can be abused; we must be transparent and only use it for
this feature. Also, any persistent overlay or notification will remind the user that focus lock is on, so they’re
aware the app is active in background.
Scalability & Support:  This is mostly on-device, so scaling is about supporting different devices and OS
versions: - iOS Screen Time API works on iOS 15+; we will require that as minimum iOS version perhaps. If a
user is on iOS 14, this feature won’t work – we can hide it for them. - Android accessibility service should
work  on  most  versions  (we’ll  target  Android  8+  likely).  Some  OEMs  (like  certain  Chinese  vendors)
aggressively kill background services; we may need to instruct users to allow our app to run in background
for reliability of the lock. - We should test with popular apps like YouTube, Instagram as blocked targets to
ensure our overlay pops up consistently.
Monitoring & Fail-safes:  In case the lock fails (e.g. our service crashed and a user got into a blocked app
without checking in), we can’t do much retroactively but we should keep the service robust. For iOS, if the
user really wants to circumvent, they could go to Settings and disable Screen Time permission for our app.
That’s fine – it’s their choice. The app should detect on launch if permission was removed and prompt them
to re-enable if they still want focus lock. Similarly, on Android, if accessibility permission is lost, the app
should show a big warning in the Focus Lock settings.
20
In conclusion, the Focus Lock feature uses  native OS capabilities  (Screen Time on iOS, Accessibility on
Android) rather than hacks, aligning with well-supported methods to achieve app blocking. This ensures the
approach is maintainable and as user-friendly as possible given platform restrictions. We give the user
control over what to lock and when, and make it secure (no unintended data leaks) and reliable so that
building the habit of daily check-ins is reinforced by this gentle enforcement mechanism.
Backend Architecture & Cloud Services
(Cross-cutting considerations for all features.)
Architecture Choice:  We favor a  serverless backend  for SOAR, given it offers easy scalability and cost
efficiency for our usage patterns. In a serverless model, we create cloud functions (or Lambdas) for specific
tasks (such as handling recommendation requests, generating podcasts, processing auth triggers, etc.) that
run only when invoked. This means we aren’t paying for idle server time and can automatically scale out to
meet demand spikes (like many users checking in each morning). For instance, if 1000 users trigger the
recommendation function at 8am, the cloud provider will run multiple instances in parallel, then scale
down.  This  pay-per-use  model  can  save  around  40%  costs  compared  to  always-on  servers  for  spiky
workloads. It also offloads maintenance (no need to manage OS or scaling manually – the provider handles
it). 
Alternatively, a  microservices architecture  with containers (on Kubernetes or similar) could give more
control and possibly cost savings at very high constant loads. But for a startup, the overhead of managing
that is high. We can however structure our serverless functions in a microservice-like way (each function or
set of functions is a “service” for a feature). They can communicate via HTTPS or events. The key point is we
do  not need a monolithic server application running 24/7. We use managed services for database, auth,
etc., which fits serverless principles (no self-hosted DB server , for example). 
Cloud Provider:  We have mentioned both Firebase (Google Cloud) and AWS. Either can achieve our needs: -
Firebase  gives  a  unified  platform:  Authentication,  Firestore  (NoSQL  DB),  Cloud  Functions  (Node.js  or
Python),  Cloud  Storage,  FCM  (notifications),  Analytics,  all  integrated.  This  speeds  up  development
considerably.  It’s  well-supported  and  can  scale  globally  (Firestore  is  multi-region  and  handles  large
throughput). -  AWS  is more granular: using Amazon Cognito for auth, API Gateway + Lambda for APIs,
DynamoDB for NoSQL, S3 for storage, Pinpoint for notifications, etc. AWS is very scalable and has more
flexibility in configuration. It might be slightly more complex to tie together , but is enterprise-grade. - We
prioritize developer efficiency but also cost: Firebase’s Blaze (pay-go) plan and AWS’s free tiers would both
let us start with minimal cost. If we assume heavy use of recommendations and such, AWS might have
some cost advantages for certain heavy tasks (like using AWS Personalize if we go that route, or cheaper
data egress for media maybe). However , using AWS fully might require more DevOps effort. - We could also
consider Azure  or others, but there’s no strong reason unless we want Azure’s AI services. Google and AWS
both have comparable offerings.
For this project, a plausible stack is Firebase  for core (since it covers mobile-centric needs well), augmented
by Google Cloud TTS  for the voice feature and perhaps some GCP Cloud Run  jobs for any scheduled tasks
not easily done in Firebase directly. Alternatively, do everything on AWS: using Amplify CLI to set up all
needed resources (Amplify is AWS’s toolkit to easily configure auth, API (AppSync or API Gateway), storage,
etc. for mobile apps). AWS Amplify could generate a GraphQL API with AppSync for realtime data, which
21
might be neat for chat (AppSync subscriptions could handle chat messages, and it can tie to DynamoDB).
But Amplify is somewhat opinionated and might be more than needed.
We’ll assume  Firebase  for simplicity: -  Firestore : stores user profiles, mood logs, group chat messages,
perhaps some cached recommendation results or content lists. Firestore is flexible, scales automatically,
and supports offline (which is nice if user has spotty connection – their mood check-in could save locally and
sync when online). - Firebase Auth : manages user login securely with minimal dev work. - Cloud Functions :
Node.js functions for endpoints (recommendations fetch, voice generation, sending daily notifications, etc.).
These can be HTTP-triggered or scheduled. They run in Google’s infrastructure and auto-scale. - Firebase
Storage : for storing any uploaded media (user avatar images, the podcast audio files). -  FCM (Firebase
Cloud Messaging) : to send push notifications (daily reminders, community messages, etc.). FCM handles
both Android and iOS nicely (iOS via APNs behind the scenes). - Analytics & Crashlytics : to monitor app
usage and stability.
Database Choices:  We touched on Firestore, but what about relational data or heavy queries? Firestore can
do a lot with document-based queries and indexes. If we needed complex joins (like matching users by
multiple criteria), we might use Firestore for most and maybe a secondary analytical DB for advanced
queries (e.g. export data periodically to BigQuery for analysis or use an Algolia for searching content
quickly). But likely not needed initially. DynamoDB (if AWS) serves a similar role as Firestore – highly scalable
NoSQL with low latency.
One might ask: do we need a SQL database ? Perhaps not for core features; all data (mood logs, content
recs,  messages)  is  well-suited  to  NoSQL  document  model.  A  SQL  DB  (like  PostgreSQL)  could  handle
relational  aspects  like  many-to-many  relationships  elegantly  (like  users-to-groups  membership),  but
Firestore can handle that with subcollections or mapping tables.
Scalability & Reliability:  By using cloud managed services, we inherit a lot of reliability. Data is replicated
(Cloud Firestore replicates data across multiple zones automatically). Functions scale concurrently – we just
must avoid heavy state or long transactions. If certain operations become hot (like all users hitting our
recommendation endpoint at 8am), we should monitor to ensure we’re not hitting any concurrency limits. If
so, we could distribute requests or increase quotas.
APIs and Microservices:  Each third-party API integration (movies, music, etc.) is a kind of microservice
consumption.  We  might  separate  these  concerns  by  writing  different  cloud  functions  for  each  and  a
coordinator function. However , that can introduce overhead. More simply, one function can call multiple
APIs. But if we foresee expanding each (like maybe a dedicated recommendation microservice in the future,
or a separate service for community to isolate chat load), microservices architecture would isolate them in
different deployments. In serverless context, it could mean different Cloud Function sets or even different
projects/microservices that communicate via REST or pub/sub. At our scale, not necessary but something to
keep in mind as we design (keeping code modular so we can break it apart later if needed).
DevOps  &  CI/CD:  With  these  choices,  deployment  is  relatively  straightforward  (Firebase  CLI  deploys
functions and rules; AWS Amplify or SAM deploys Lambdas, etc.). We should set up a CI pipeline on GitHub/
GitLab such that pushing changes auto-builds and deploys to avoid downtime. Testing functions locally is
possible with emulators (Firebase emulator suite for Firestore/Functions is helpful for dev).
22
Security (Backend):  We enforce auth on all endpoints – e.g. Firebase Functions can automatically require
Firebase Auth context; if not using Firebase, we would require including a JWT from Cognito or similar .
Communication is over HTTPS by default on these cloud endpoints. For database, implement security rules
(in Firestore) to restrict read/write as appropriate. We will also use Firestore’s security rules  to ensure, for
example, users can only write mood logs to their own document, only read messages of groups they belong
to, etc. These rules run on server side for every access, adding a strong security layer in addition to our
function logic.
Cost Consideration:  We choose services that have free tiers for initial usage: - Firebase free tier covers a
generous  amount  of  Auth  (like  10k  users),  Firestore  (50k  reads/day  etc.),  and  125k  Cloud  Function
invocations per month. This should cover our early usage. As we scale, costs will ramp linearly, but we can
optimize (cache results to cut external API calls and Firestore reads, etc.). We’ll also monitor usage to catch
any abuse or inefficiency (like if our chat message design was too read-intensive). - Third-party APIs: TMDB
is free but requires attribution. Yelp’s new API might have costs if heavy usage (but we can limit or consider
alternatives if that becomes expensive). Most others (YouTube, Apple Music, etc.) are free within normal
use. - TTS costs we discussed – manageable if user base is moderate. If we scaled to millions of users all
getting daily podcasts, we’d evaluate maybe deploying an open source TTS model on our own server to
avoid per-call charges, but that’s heavy (and quality might suffer). Using the cloud might still be fine due to
volume discounts or just factoring it into subscription price if we ever charge users. - In serverless, one
hidden cost can be cold starts and response times, but if coded efficiently and with languages like Node/
Python, it’s usually fine. We might keep some functions warm by periodic ping if needed for performance.
Logging & Monitoring:  We will utilize cloud monitoring tools to log important events (e.g. function errors,
latencies). This helps debug issues in production quickly. Crashlytics on client plus logs on backend give full
visibility. We also might log significant user actions to an analytics DB (like BigQuery) for future analysis of
feature effectiveness (with user consent as needed).
Conclusion:  The technology choices and architecture outlined ensure that SOAR’s features are built on
scalable, reliable, and secure  foundations. By using cross-platform development on the client and serverless
cloud services on the backend, we minimize reinventing the wheel and leverage well-supported platforms.
This allows the development team to focus on delivering a polished experience that can grow with our
userbase, all while keeping costs in check and maintaining a high level of data security.
Security & Privacy Considerations
(Summarizing data handling practices across the app.)
SOAR deals with sensitive personal data (mood, mental health info, community conversations). Therefore,
security and privacy are paramount: -  Data Encryption:  All network traffic is encrypted via HTTPS TLS.
Sensitive  data  stored  on  device  (like  cached  mood  entries  or  authentication  tokens)  will  be  stored  in
encrypted form if possible (for example, use iOS Keychain/Android Keystore for tokens). On the server , user
data in databases is encrypted at rest by cloud provider default (Firestore, DynamoDB, S3 all do this). We will
additionally encrypt particularly sensitive fields (e.g. mental health history notes) on the application level if
deemed necessary (so even our DB admins can’t easily read them without keys). - Authentication & Access
Control:  Using robust auth (Firebase Auth/Cognito) ensures each user has a unique identity token. We
enforce access rules so users can only access their own data. In the community, we use group-based access
rules. All API calls verify the user token. We will also use refresh tokens and secure storage to manage
23
sessions safely. Multi-factor auth can be an option via Firebase if we want extra security for login (optional
to  the  user).  -  Minimize  Data  Collection:  We  only  ask  for  information  that  directly  benefits  the  user
experience (age, preferences, etc., used for personalization). We do not sell or share user data. If we
integrate any third-party that requires data (like sending a query to AI for content), we’ll transparently
inform users and anonymize data as much as possible. - Compliance:  While not a regulated medical app,
we aim to follow practices aligned with HIPAA and GDPR. That means giving users control: they can delete
their account and all associated data (we’ll implement a deletion function that removes their records from
Firestore/Dynamo, storage, etc.). We’ll have a clear privacy policy and terms explaining data usage. We also
ensure our vendors (like cloud providers) are compliant with relevant standards (Firebase and AWS are
HIPAA and GDPR capable provided we configure them properly). If the user is in the EU, we’ll store data in
EU data centers if needed (Firestore can be configured to a region). - Monitoring & Alerts:  Security isn’t just
encryption. We’ll monitor for unusual activities (like failed login attempts, indicating possible attacks). Rate-
limit critical endpoints to prevent abuse (like someone spamming our recommendation API or trying to
brute force something). - App Security:  We will keep the app code secure (avoid embedding secrets – all API
keys that need to be secret will reside in the backend). We will also employ certificate pinning if appropriate
to prevent MITM attacks on mobile (though using HTTPS and proper CA validation is usually enough). -
Personal Safety:  Given the nature of the app, we will include resources and perhaps emergency contact
info in-app for users who might need professional help. The community will be moderated to ensure a safe
environment (no bullying, etc.). This is less technical security and more user safety, but equally important. -
Permissions Transparency:  The app will request potentially intrusive permissions (Accessibility, Device
Admin, Location, Health data maybe). We will clearly explain why each is needed and allow opting out of
features that require them. For instance, if user doesn’t want to share location, they can still use the app
(just no local therapist recommendations, etc.). We ensure the app still functions and doesn’t force data
sharing beyond comfort.
By addressing these points, we instill trust in users that their data is safe with SOAR. Trust is crucial for an
emotional wellness app – users will only engage deeply if they feel secure. We will regularly review our
security measures and update them as new threats or best practices emerge.
Conclusion
In building the SOAR mobile app, we combine cutting-edge AI  (for voice and intelligent suggestions) with
proven app frameworks and cloud services  to create a personalized wellness companion. Each feature –
from daily mood check-ins with focus locks, to multi-faceted recommendations, to AI voice podcasts and
supportive community chat – is designed with scalability, privacy, and user experience in mind. We leverage
well-supported  APIs  (TMDB,  YouTube,  Apple  Music,  etc.)  to  enrich  content,  and  robust  ML  models  for
personalization  and  voice  synthesis,  all  orchestrated  through  a  cost-effective,  serverless  backend.  The
frontend uses modern cross-platform UI toolkit (Flutter/React Native) to deliver a seamless experience on
iOS and Android, tapping into native capabilities for critical functionality (like Apple’s Screen Time for app
blocking). 
By prioritizing  well-supported technologies  – e.g. Flutter (now one of the most popular mobile SDKs),
Firebase/AWS  cloud,  and  reputable  third-party  APIs  –  we  ensure  that  SOAR  can  be  developed  rapidly
without sacrificing quality, and can reliably serve a growing user base. The architecture is cloud-native and
modular , making it easy to iterate on features (for instance, adding new recommendation sources or new
community matching algorithms) without major refactoring.
24
Crucially, throughout the technical design, we maintain a  user-centric, professional approach : data is
handled securely, features are implemented in a way that respects the user’s time and privacy, and the
app’s tone (even in AI-generated content) remains compassionate and positive. By following this guide,
developers  can  implement  the  SOAR  app  as  a  robust,  scalable,  and  uplifting  platform  for  emotional
wellness and productivity, helping users “Soar” to new heights of mental well-being  with the support of
smart technology and human connection.
Sources:  The above design leverages insights from official platform documentation and industry practices,
such as Apple’s Screen Time API usage for app blocking, the extensive content available via TMDB’s API for
movies, the cross-platform capabilities of Flutter for high-performance mobile apps, and the realism of
modern text-to-speech voices provided by Google and Amazon’s AI, which enable natural, soothing audio
experiences. These technologies collectively empower the development of SOAR into a comprehensive
wellness app. 
MusicKit - Apple Developer
https://developer .apple.com/musickit/
Getting Started with the Yelp Places API
https://docs.developer .yelp.com/docs/places-intro1
2
25
